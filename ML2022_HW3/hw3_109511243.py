# -*- coding: utf-8 -*-
"""hw3_109511243.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qsh04TVhZjNhBbcrBv8f6diBHlf9Aa6G
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import os
from sklearn import svm
from scipy.stats import multivariate_normal


# you can choose one of the following package for image reading/processing

import cv2
import PIL

"""#**1. Support Vector Machine**

##**2.OVR OVO**  
**one versus the rest:** 如果有N個class，會製造N個model 每個model i會判斷x是否屬於class i,但是如果出現多個model都認定x屬於該class，或者不屬於任何class就需要額外的criterion處理  

**one versus one:** 如果有N個class，會製造(N*(N-1)/2)個model 每個分類0v1,0v2....0vN,N-1vN 然後根據每個model的結果投票最後選擇票數最高的選項，需要特別處裡的情況只有最高票數的class有多個，可以引入額外的criterion進行選擇，也可以很直覺的任選其中一個就好，因為都是最有可能的選項  

**選擇使用one versus one:**相較於OVR,需要引入額外的criterion處理各種例外情況,OVO的例外處理相對簡單而且也相對直覺，model數更多可能也會提供相對穩定的結果，所以這邊使用OVO

##**Linear Kernel**
"""

from google.colab import drive
drive.mount('/content/drive')
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/x_train.csv",header= None)/255
label = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/data/t_train.csv",header= None)

data_numpy=data.to_numpy()
label_numpy=label.to_numpy()

"""check if data is correct"""

plt.imshow(data_numpy[0].reshape((28,28)),cmap="gray")

def PCA(data, d):
  new_data = data - np.mean(data, axis=0)
  U, Sigma, Vh = np.linalg.svd(new_data.T)
  pca_data = np.dot(new_data, U[:,:d])
  return pca_data

def linear_kernel(x1,x2):
    return (x1.reshape(1,-1)@x2.reshape(-1,1)).item()
def linear_phi(x):
    return x
def poly_phi(x):
    x1,x2=x
    return np.array([x1**2,1.4142135*x1*x2,x2**2])
def poly_kernel(x1,x2):
    return ((x1.reshape(1,-1)@x2.reshape(-1,1)).item())**2

data_pca=PCA(data_numpy,2)
clf=svm.SVC(kernel='linear',degree=2,C=1,decision_function_shape='ovo')
clf.fit(data_pca,label.to_numpy().ravel())
#print(clf.score(data_pca,label.to_numpy().ravel()))

"""**Building three model**  
Find suppoert vectors and corresponding alpha given by sklearn
"""

#index 0 for 0v1, 1 for 0v2 2 for 1v2
v=(clf.support_vectors_)
[len_0,len_1,len_2]=clf.n_support_

"""
#ndarray
vector_0=v[0:len_0]
vector_1=v[len_0:(len_0+len_1)]
vector_2=v[-5:]
"""
#list
vector_0=[i for i in v[0:len_0]]
vector_1=[i for i in v[len_0:(len_0+len_1)]]
vector_2=[i for i in v[-5:]]
support_vectors=[vector_0,vector_1,vector_2]

dual_coef=clf.dual_coef_
alpha_01=dual_coef[0][0:(len_0+len_1)]
alpha_02=np.concatenate([dual_coef[1][0:len_0],dual_coef[0][-len_2:]])
alpha_12=np.concatenate([dual_coef[1][len_0:(len_0+len_1)],dual_coef[1][-len_2:]])
alpha_list=[alpha_01,alpha_02,alpha_12]

#index 0 for 0v1, 1 for 0v2 2 for 1v2
def get_weight(alpha_list,support_vectors,kernel="linear"):
    if kernel=="linear":
        kernel=linear_kernel
        phi=linear_phi
    else:
        kernel=poly_kernel
        phi=poly_phi
    weight_list=[0]*3
    #model0 0v1
    for i,vec in enumerate(support_vectors[0]+support_vectors[1]):
        weight_list[0]+=(phi(vec)*alpha_list[0][i])
    #model1 0v2
    for i,vec in enumerate(support_vectors[0]+support_vectors[2]):
        weight_list[1]+=(phi(vec)*alpha_list[1][i])
    #model2 1v2
    for i,vec in enumerate(support_vectors[1]+support_vectors[2]):
        weight_list[2]+=(phi(vec)*alpha_list[2][i])
    return weight_list

#M for <1  precisely <C given C=1
#S for <=1
def get_bias(alpha_list,support_vectors,kernel="linear",C=1):
    if kernel=="linear":
        kernel=linear_kernel
        phi=linear_phi
    else:
        kernel=poly_kernel
        phi=poly_phi
    bias_list=[0]*3
    subset_list=[support_vectors[0]+support_vectors[1],
                 support_vectors[0]+support_vectors[2],
                 support_vectors[1]+support_vectors[2]]
    #model 0 for 0v1, 1 for 0v2 2 for 1v2
    for model, subset in enumerate(subset_list):     
        countM=0
        for i,vec1 in enumerate(subset):
            if alpha_list[model][i]==0:
                continue
            if abs(alpha_list[model][i])==C: #M for 0< <1
                continue
            countM+=1
            tn=np.sign(alpha_list[model][i]) 
            S_sum=0
            for j,vec2 in enumerate(subset): #S for <=1
                S_sum+=(kernel(vec1,vec2)*alpha_list[model][j])
            bias_list[model]+=tn-S_sum.item() 
        bias_list[model]/=countM
    return bias_list

weight_list=get_weight(alpha_list,support_vectors,kernel="linear")
print(weight_list)
#print(clf.coef_) #for check
bias_list=get_bias(alpha_list,support_vectors,kernel="linear")
print(bias_list)
#print(clf.intercept_) #for check

def predict(X,weight_list,bias_list): #X.shape shoudl be(N,D)
    """
    Parameters
    ----------
    X: shape shoudl be(N,D) D is dimension of phi(xi)
    weight_list: list of weight(D,) for each model
    bias_list: list of bias(scalar) for each model
    #model's amount = choices*(choices-1)/2

    Returns
    -------
    predict_label:shape(N,) 
    """
    N=len(X)
    vote=np.zeros((N,3))
    y0=weight_list[0].reshape(1,-1)@X.T+bias_list[0]  #1,300
    y1=weight_list[1].reshape(1,-1)@X.T+bias_list[1]
    y2=weight_list[2].reshape(1,-1)@X.T+bias_list[2]
    for i in range(N):
        if y0[0][i]>0:
            vote[i][0]+=1
        else:
            vote[i][1]+=1
        if y1[0][i]>0:
            vote[i][0]+=1
        else:
            vote[i][2]+=1
        if y2[0][i]>0:
            vote[i][1]+=1
        else:
            vote[i][2]+=1
    return np.argmax(vote,axis=1)

"""**Ploting Stage**"""

#reference:https://github.com/andy6804tw/2021-13th-ironman/blob/main/11.SVM/11.1.SVM(Classification-iris).ipynb
def make_meshgrid(x, y, h=.02):
    """Create a mesh of points to plot in

    Parameters
    ----------
    x: data to base x-axis meshgrid on
    y: data to base y-axis meshgrid on
    h: stepsize for meshgrid, optional

    Returns
    -------
    xx, yy : ndarray
    """
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy

subdata_list=[]
for i in range(3):
    index=np.where(label_numpy.reshape(-1) == i)
    subdata_list.append(data_pca[index])

X0, X1 = data_pca[:, 0], data_pca[:, 1]
x_, y_ = make_meshgrid(X0, X1) #坐標軸每個點(顏色解析度) ex:在(10,10)中平均取1000或10000個點
#cotour
points=np.c_[x_.ravel(), y_.ravel()]
predictions=predict(points,weight_list,bias_list)

#for i in range(len(points)):
#    predictions.append(predict(points[i],weight_list,bias_list))
N,D=x_.shape
Z=np.array(predictions).reshape((N,D))
plt.contourf(x_, y_,Z,cmap=plt.cm.brg, alpha=0.1)
#scatter

plt.scatter(subdata_list[0][:,0], subdata_list[0][:,1],s=20, c='b', marker='x', label="T-shirt/top")
plt.scatter(subdata_list[1][:,0], subdata_list[1][:,1],s=20, c='r', marker='x', label="Trouser")
plt.scatter(subdata_list[2][:,0], subdata_list[2][:,1],s=20, c='g', marker='x', label="Sandal")
plt.scatter(v[:,0], v[:,1],s=90, facecolors='none', edgecolors='k', linewidths=1,label="Support Vector")

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.title('SVM (linear kernel)')
fig = plt.gcf()
fig.set_size_inches(15, 10)

"""##**Polynomial (degree=2)**"""

clf=svm.SVC(kernel='poly',degree=2,C=1,decision_function_shape='ovo')
clf.fit(data_pca,label.to_numpy().ravel())
#print(clf.score(data_pca,label.to_numpy().ravel()))
print(clf.n_support_)

#index 0 for 0v1, 1 for 0v2 2 for 1v2
v=(clf.support_vectors_)
[len_0,len_1,len_2]=clf.n_support_

"""
#ndarray
vector_0=v[0:len_0]
vector_1=v[len_0:(len_0+len_1)]
vector_2=v[-5:]
"""
#list
vector_0=[i for i in v[0:len_0]]
vector_1=[i for i in v[len_0:(len_0+len_1)]]
vector_2=[i for i in v[-len_2:]]
support_vectors=[vector_0,vector_1,vector_2]

dual_coef=clf.dual_coef_
alpha_01=dual_coef[0][0:(len_0+len_1)]
alpha_02=np.concatenate([dual_coef[1][0:len_0],dual_coef[0][-len_2:]])
alpha_12=np.concatenate([dual_coef[1][len_0:(len_0+len_1)],dual_coef[1][-len_2:]])
alpha_list=[alpha_01,alpha_02,alpha_12]

weight_list=get_weight(alpha_list,support_vectors,kernel="poly")
bias_list=get_bias(alpha_list,support_vectors,kernel="poly")
print(weight_list)
print(bias_list)

X0, X1 = data_pca[:, 0], data_pca[:, 1]
x_, y_ = make_meshgrid(X0, X1) #坐標軸每個點(顏色解析度) ex:在(10,10)中平均取1000或10000個點
#cotour
points=np.c_[x_.ravel()*x_.ravel(),1.4142135*x_.ravel()*y_.ravel() ,y_.ravel()*y_.ravel()]
predictions=predict(points,weight_list,bias_list)

#for i in range(len(points)):
#    predictions.append(predict(points[i],weight_list,bias_list))
N,D=x_.shape
Z=np.array(predictions).reshape((N,D))
plt.contourf(x_, y_,Z,cmap=plt.cm.brg, alpha=0.1)
#scatter

plt.scatter(subdata_list[0][:,0], subdata_list[0][:,1],s=20, c='b', marker='x', label="T-shirt/top")
plt.scatter(subdata_list[1][:,0], subdata_list[1][:,1],s=20, c='r', marker='x', label="Trouser")
plt.scatter(subdata_list[2][:,0], subdata_list[2][:,1],s=20, c='g', marker='x', label="Sandal")
plt.scatter(v[:,0], v[:,1],s=90, facecolors='none', edgecolors='k', linewidths=1,label="Support Vector")

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.title('SVM (poly kernel)')
fig = plt.gcf()
fig.set_size_inches(15, 10)

"""#**2. Gaussian Mixture Model**

##**K-mean**
"""

img = cv2.imread("/content/drive/MyDrive/Colab Notebooks/data//hw3.jpg") #read into BGR

def dist(p1,p2):
    return np.dot(p1-p2,p1-p2)

def K_mean(img,K,d=3):

    means = np.zeros((K, d))
    N, M, _ = img.shape
    mean_list = [[0, 0, 0] for i in range(K)]
    sum_list = [[0, 0, 0] for i in range(K)]
    count_list = [0 for i in range(K)]
    assign_map = np.zeros((N, M))

    mean_list = [np.zeros(3) for i in range(K)]
    for i in range(K):
        mean_list[i]=np.array([255/(K+1)*i]*3)
    print(mean_list)
    iteration = 50
    for t in range(iteration):
        sum_list = [np.zeros(3) for i in range(K)]
        count_list = [0 for i in range(K)]
        last_mean=mean_list
        for i in range(N):
            for j in range(M):
                cur_dist = [0 for i in range(K)]
                for k in range(K):
                    cur_dist[k] = dist(img[i][j], mean_list[k])

                assignment = np.argmin(cur_dist)
                assign_map[i][j] = assignment
                sum_list[assignment] += img[i][j]
                count_list[assignment] += 1

        mean_list = [sum_list[i]/(count_list[i]+0.000001) for i in range(K)] #in case of divide by 0
        
        cur_sum=0
        for i in range(K):
            cur_sum+=abs(np.sum(last_mean[i]-mean_list[i]))
        #print(cur_sum)
        if cur_sum==0:
            break
        last_mean=mean_list #don't need list.copy()?
        #print(mean_list)
        
    img_K=np.zeros((N,M,3))
    for i in range(N):
        for j in range(M):
            img_K[i][j]=mean_list[int(assign_map[i][j])]
    img_K=img_K.astype("int32")
    print(mean_list)
    return img_K,mean_list,assign_map

"""
img_K5=np.zeros((N,M,3))
for i in range(N):
    for j in range(M):
        img_K5[i][j]=mean_list[int(assign_map[i][j])]
img_K5=img_K5.astype("int32")
"""
img_list=[]
means_list=[]
k_list=[2,3,7,20]
for k in k_list:
    img_k,means,_=K_mean(img,k)
    img_list.append(img_k)
    means_list.append(means)

fig, ax = plt.subplots(2,2)

for i in range(4):
    ax[i//2,i%2].set_title("K = {}".format(k_list[i]))
    ax[i//2,i%2].imshow(img_list[i][:,:,::-1]) #BGR to RGB
    ax[i//2,i%2].axis("off")
"""
i+=1
ax[i//2,i%2].set_title("original")
ax[i//2,i%2].imshow(img[:,:,::-1])
ax[i//2,i%2].axis("off")
"""
fig.set_size_inches(15, 10)
plt.show()
cur=0
for k in k_list:
    print("-----------means of K={}---------".format(k))
    l=means_list[cur]
    for mean in l:
        print(mean)
    cur+=1

"""##**GMM**"""

def respons(X,pi_list,mean_list,cov_list): #return R(Zn~) [k=0,k=1.......]
    #X(N,3)
    K=len(mean_list)
    N=len(X)
    pdf_list=[]
    pdf_sum=np.zeros(N)

    for i in range(K):
        pdf=pi_list[i]*multivariate_normal.pdf(X,mean_list[i],cov_list[i],allow_singular=True)
        pdf_list.append(pdf)
        pdf_sum+=pdf
    R=np.array(pdf_list)
    return np.divide(R,pdf_sum).T
def log_likelihood(X,pi_list,mean_list,cov_list):
    K=len(mean_list)
    N=len(X)
    pdf_list=[]
    pdf_sum=np.zeros(N)

    for i in range(K):
        pdf=pi_list[i]*multivariate_normal.pdf(X,mean_list[i],cov_list[i],allow_singular=True)
        pdf_list.append(pdf)
    R=np.array(pdf_list)  #(K,N)
    R=R.T
    return np.sum(np.log(np.sum(R,axis=1)))

#用矩陣乘法寫比迴圈快差不多100倍
def GMM(img,mean_list,K,iteration=100):
  N,M,_ = img.shape
  pi_list = [0]*K
  data_subset=[[] for _ in range(K)]
  #INITIAL PI
  for i in range(N):
      for j in range(M):
          cur_dist = [0 for i in range(K)]
          for k in range(K):
              cur_dist[k] = dist(img[i][j], mean_list[k])
          assignment = np.argmin(cur_dist)
          pi_list[assignment]+=1
          data_subset[assignment].append(img[i][j])
  data_subset=[np.array(d) for d in data_subset]

  #INITIAL COVARIANCE      
  pi_list=[pi/(N*M) for pi in pi_list]
  cov_list=[0]*K
  for k in range(K):
      tmp=data_subset[k].reshape(-1,3)
      cov_list[k]=np.cov(tmp.T)
  #print(cov_list)

  #EM
  likelihood_list=[]
  for i in range(iteration):
    pixels=img.reshape(-1,3)
    #RESPONSIBILITY
    R=respons(pixels,pi_list,mean_list,cov_list)

    #MEAN an PI
    mean_list=[np.sum((R[:,k]*pixels.T).T,axis=0) for k in range(K)]
    count_list=np.sum(R,axis=0)

    mean_list=[mean_list[i]/count_list[i] for i in range(K)]
    pi_list=count_list/sum(count_list)
    #print(mean_list)

    #COVARIANCE
    vec_list=[pixels-mean_list[k] for k in range(K)]
    cov_list=[R[:,k]*vec_list[k].T@vec_list[k]  for k in range(K)]
    cov_list=[cov_list[k]/count_list[k] for k in range(K)]
    
    #LIKELIHOOD
    likelihood_list.append(log_likelihood(pixels,pi_list,mean_list,cov_list))
    #print("log likelihood:",likelihood_list[-1])
  print("--------------K={}-------------".format(K))
  print(mean_list)
  print(likelihood_list[-1])
  return  mean_list,likelihood_list

K_list=[2,3,7,20]
GMM_means_list=[]
likelihood_list=[]
for i,k in enumerate(K_list):
    means,likeli=GMM(img,means_list[i],k)
    GMM_means_list.append(means)
    likelihood_list.append(likeli)

"""**Learning Curve**"""

fig, ax = plt.subplots(2,2)

for i in range(4):
    ax[i//2,i%2].set_title("K = {}".format(k_list[i]))
    ax[i//2,i%2].plot(range(len(likelihood_list[i])),likelihood_list[i],marker='o', color='blue',label="log likelihood")

fig.set_size_inches(15, 10)
plt.show()
cur=0
for k in k_list:
    print("-----------means of K={}---------".format(k))
    l=GMM_means_list[cur]
    for mean in l:
        print(mean)
    cur+=1

"""**Image of GMM**"""

def img_K(img,mean_list):
  N,M,_=img.shape
  ret=np.zeros((N,M,3))
  K=len(mean_list)
  for i in range(N):
    for j in range(M):
      cur_dist = [0 for i in range(K)]
      for k in range(K):
          cur_dist[k] = dist(img[i][j], mean_list[k])
      assignment = np.argmin(cur_dist)
      ret[i][j]=mean_list[assignment]
  return ret.astype("int32")

fig, ax = plt.subplots(2,2)

for i in range(4):
    ax[i//2,i%2].set_title("K = {}".format(k_list[i]))
    ax[i//2,i%2].imshow(img_K(img,GMM_means_list[i])[:,:,::-1]) #BGR to RGB
    ax[i//2,i%2].axis("off")
fig.set_size_inches(15, 10)
plt.show()

"""##4.Discussion
雖然兩種方法製造出的image乍看之下很相仿，但是仔細觀察會發現K-means的圖飽和度相對大，顏色的對比明顯，而GMM的顏色對比相較之下沒有那麼明顯，視覺效果相對柔和。  
可能是因為K_means是minimize pixel的distant 所以為了顧慮每個pixel(包括outlier)會得出平均分布且距離相對遙遠的means，距離遙遠的means就會產生顏色對比度大的視覺效果。
而GMM是最大化機率所以mean就會向大部分pixel的平均值靠攏，outlier的影響就會被降低，所以最後得到的mean就會是跟大部分pixel相近的顏色，產生顏色對比度小的柔和效果

#廢棄的code
"""

subdata_list=[]
subtarget_list=[]
for i in range(3):
    subdata_list.append(data[label[0]==i])    #label==0 不行 (是dataframe不是series)
    subdata_list[i]=PCA(subdata_list[i].to_numpy(),2)
    N,D=subdata_list[i].shape
    subtarget_list.append(np.array([i for j in range(N)]))

t1=data[data[3]==0]
t2=data[label[0]==0] #label==0 不行 (是dataframe不是series)
t3=(data[3]==0)
t4=(label[0]==0)

#Using full Matrix multiplication is more appropriate
def predict(x,weight_list,bias_list):
    vote=np.zeros(3)
    y0=weight_list[0].reshape(1,-1)@x+bias_list[0]
    if y0>0:
        vote[0]+=1
    else:
        vote[1]+=1
    y1=weight_list[1].reshape(1,-1)@x+bias_list[1]
    if y1>0:
        vote[0]+=1
    else:
        vote[2]+=1
    y2=weight_list[2].reshape(1,-1)@x+bias_list[2]
    if y2>0:
        vote[1]+=1
    else:
        vote[2]+=1
    return np.argmax(vote)
for i in range(10):
    print(predict(data_pca[i],weight_list,bias_list))

"""
好像太平均
# initial assign
for i in range(N):
    for j in range(M):
        assign_map[i][j] = i % K
        assignment = int(assign_map[i][j])  # numpy to int
        sum_list[assignment] += img[i][j]
        count_list[assignment] += 1
mean_list = [sum_list[i]/count_list[i] for i in range(K)]
print(mean_list)
"""

K=3
mean_list=means_list[1]
N,M,_ = img.shape
pi_list = [0]*K
data_subset=[[] for _ in range(K)]

for i in range(N):
    for j in range(M):
        cur_dist = [0 for i in range(K)]
        for k in range(K):
            cur_dist[k] = dist(img[i][j], mean_list[k])
        assignment = np.argmin(cur_dist)
        pi_list[assignment]+=1
        data_subset[assignment].append(img[i][j])
data_subset=[np.array(d) for d in data_subset]
#initial covariance       
pi_list=[pi/(N*M) for pi in pi_list]
cov_list=[0]*K
for k in range(K):
    tmp=data_subset[k].reshape(-1,3)
    cov_list[k]=np.cov(tmp.T)
print(cov_list)

#EM
likelihood_list=[]
pixels=img.reshape(-1,3)

print(mean_list)
#response
R=respons(pixels,pi_list,mean_list,cov_list)
#mean and pi
mean_list=[np.sum((R[:,k]*pixels.T).T,axis=0) for k in range(K)]
count_list=np.sum(R,axis=0)

mean_list=[mean_list[i]/count_list[i] for i in range(K)]
pi_list=count_list/sum(count_list)
print(mean_list)

#cov
vec_list=[pixels-mean_list[k] for k in range(K)]
cov_list=[R[:,k]*vec_list[k].T@vec_list[k]  for k in range(K)]
cov_list=[cov_list[k]/count_list[k] for k in range(K)]
print(cov_list)

log_likelihood(pixels,pi_list,mean_list,cov_list)