{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ma3v99vDrx6y","outputId":"4c04edbe-0ba1-42bb-dfd5-9dec76df6ebc","executionInfo":{"status":"ok","timestamp":1666007782586,"user_tz":-480,"elapsed":17517,"user":{"displayName":"蕭登方","userId":"04110943827274540537"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","wine_data=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/X.csv\")\n","wine_target=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/T.csv\")\n","\n","wine_features=[\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"]\n","X=wine_data[wine_features]\n","Y=wine_target"],"metadata":{"id":"7rhdlLgDt7AA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After observing the distribution of each feature with their corresponding target.(by scatter plot)  \n","and regression result with each single feature  \n","I found almost every features have a linear-like distribution,  \n","Thus, I choose polynomial functions as the basis funcions for my regression process "],"metadata":{"id":"1Zs1Rn86dkTo"}},{"cell_type":"markdown","source":["**----------------------Below is the Function define stage-------------------------**"],"metadata":{"id":"W5iD3mYdf2v3"}},{"cell_type":"code","source":["def split_DataFrame(X,N): #Can't handle Series\n","    df_1=X.iloc[:N,:]\n","    df_2=X.iloc[N:,:]\n","    return df_1, df_2\n","def variance_numpy(W0,W,X,Y):\n","    N,D=X.shape\n","    count=0\n","    for i in range(N):\n","        count+=(predict(W0, W, X[i])-Y[i])**2\n","    count/=N\n","    return count\n","def gaussian(x,u,var):\n","    deviation=np.sqrt(var)\n","    tmp=np.sqrt(2*np.pi)\n","    probability=np.exp(-(x-u)**2/(2*var))/(deviation*tmp)\n","    return probability\n","def predict(W0,W1,X):\n","    X=X.reshape(len(X),1)\n","    return np.matmul(W1,X)+W0\n","def regression_M(X,Y,M):\n","    X_numpy=X.to_numpy()\n","    H=X_numpy\n","    N, D=X_numpy.shape\n","    i=0\n","    for m in range(1,M):\n","        coefs=H.size/N\n","        while i<coefs :\n","            for j in range(D):\n","                tmp=H[:,i]*X_numpy[:,j]\n","                tmp=tmp.reshape(N,1)\n","                H=np.append(H,tmp,1)\n","                #print(H.shape)\n","            i+=1\n","    \n","    ones=np.array([[1]]*N)\n","    H=np.append(H,ones,1)\n","    Y_numpy=Y.to_numpy()\n","    H_trans=np.transpose(H)\n","    mul=np.matmul(H_trans,H)\n","    tmp=np.linalg.pinv(mul)\n","    W=np.matmul(np.matmul(tmp,H_trans),Y_numpy)\n","    intercept=W[-1]\n","    W=np.delete(W,-1)\n","    H=np.delete(H,-1,1)\n","    return intercept, W, H\n","def polynomial_form_M(X, M):\n","    X_numpy=X.to_numpy()\n","    H=X_numpy\n","    N, D=X_numpy.shape\n","    i=0\n","    for m in range(1,M):\n","        coefs=H.size/N\n","        while i<coefs :\n","            for j in range(D):\n","                tmp=H[:,i]*X_numpy[:,j]\n","                tmp=tmp.reshape(N,1)\n","                H=np.append(H,tmp,1)\n","                #print(H.shape)\n","            i+=1\n","    return H"],"metadata":{"id":"nqY1qW0IuFHU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below are the maximize likelihood process with different order of polynomial regression.  \n","Because the probability is to low to show even for float64,  \n","So here I introduce log likelihood with base e\n"],"metadata":{"id":"pRNXlTsIgDnH"}},{"cell_type":"code","source":["X_train, X_test=split_DataFrame(X,1500)\n","Y_train, Y_test=split_DataFrame(Y,1500)\n","X_train_numpy=X_train.to_numpy()\n","Y_train_numpy=Y_train.to_numpy()\n","X_test_numpy=X_test.to_numpy()\n","Y_test_numpy=Y_test.to_numpy()\n","\n","\n","for m in range(1,4):\n","  W0, W, H_train=regression_M(X_train,Y_train,m)\n","  var=variance_numpy(W0, W, H_train, Y_train_numpy)\n","  log_likelihood=np.float64(0)\n","  \n","#--------Testing stage------------ \n","  H_test=polynomial_form_M(X_test,m)\n","  N,D=H_test.shape\n","  for i in range(N):\n","      p=gaussian(Y_test_numpy[i],predict(W0,W,H_test[i]),var)\n","      log_likelihood+=np.log(p)\n","  print(\"M=\",m, \"avg log_likelihood of test\",log_likelihood/N)\n","  log_likelihood=np.float64(0)\n","  for i in range(N):\n","      p=gaussian(Y_train_numpy[i],predict(W0,W,H_train[i]),var)\n","      log_likelihood+=np.log(p)\n","  print(\"M=\",m, \"avg log_likelihood of train(first 99))\",log_likelihood/N)\n","  print(\"----------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VH05SKufwnUZ","outputId":"67930358-c497-4b33-916b-f125575277fc","executionInfo":{"status":"ok","timestamp":1666007969450,"user_tz":-480,"elapsed":7553,"user":{"displayName":"蕭登方","userId":"04110943827274540537"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["M= 1 avg log_likelihood of test [-0.89451762]\n","M= 1 avg log_likelihood of train(first 99)) [-0.99533061]\n","----------------------------------------\n","M= 2 avg log_likelihood of test [-1.06810252]\n","M= 2 avg log_likelihood of train(first 99)) [-0.8851133]\n","----------------------------------------\n","M= 3 avg log_likelihood of test [-1.85180558]\n","M= 3 avg log_likelihood of train(first 99)) [-0.78883154]\n","----------------------------------------\n"]}]},{"cell_type":"markdown","source":["The result above shows that although the training likelihood of the higher order perform well  \n","but the likelihood of test decrease sharply  \n","over-fitting problem seems to occur when we try to increase the order of polynomial regression"],"metadata":{"id":"KLnIIlvfh7YJ"}},{"cell_type":"markdown","source":["Below I try to do N-cross validation to obtain more precise outcome  \n","\n","for deciding which order of regression is more suitable for maximize likelihood without over-fitting\n","  \n","Here I changing hyperparameter(order)"],"metadata":{"id":"6TWXJR1Ojci0"}},{"cell_type":"code","source":["def Validation_MLE_poly(X_train,X_test,Y_train,Y_test,order):\n","  X_train_numpy=X_train.to_numpy()\n","  Y_train_numpy=Y_train.to_numpy()\n","  X_test_numpy=X_test.to_numpy()\n","  Y_test_numpy=Y_test.to_numpy()\n","  ret=[]\n","\n","  for m in range(1,order+1):\n","    W0, W, H_train=regression_M(X_train,Y_train,m)\n","    var=variance_numpy(W0, W, H_train, Y_train_numpy)\n","    \n","#--------Testing stage------------ \n","    log_likelihood=np.float64(0)\n","    H_test=polynomial_form_M(X_test,m)\n","    N,D=H_test.shape\n","    for i in range(N):\n","      p=gaussian(Y_test_numpy[i],predict(W0,W,H_test[i]),var)\n","      log_likelihood+=np.log(p)\n","    log_likelihood/=N\n","    print(\"M=\",m, \"avg log_likelihood\",log_likelihood)\n","    ret.append(log_likelihood)\n"," \n","  return ret"],"metadata":{"id":"2aU2JwfM-t1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def N_set(data,cut_size):\n","  data_set=[]\n","  N,D=data.shape\n","  cur=0\n","  while cur<(N-cut_size):\n","    data_set.append(data.iloc[cur:cur+cut_size,:])\n","    cur+=cut_size\n","    \n","  data_set.append(data.iloc[cur:,:])\n","  return data_set"],"metadata":{"id":"THNszg323tuM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After defining the function for N_cross validation  \n","\n","Below is the N_cross validation of each subset have the size=100  \n","we execute the polynomial regression only for M=1~3  \n","because M=4 is too large to handle(16105 basis)"],"metadata":{"id":"-LHpGN-vkPVP"}},{"cell_type":"code","source":["cut_size=100\n","Max_poly_order=3\n","\n","X_sets=N_set(X,cut_size)\n","Y_sets=N_set(Y,cut_size)\n","times=len(X_sets)\n","\n","count=[np.float64(0)]*Max_poly_order\n","for t in range(times):\n","  print(\"----------------time=\",t,\"-------------------------\")\n","  X_train=pd.DataFrame()\n","  Y_train=pd.DataFrame()\n","  X_test=X_sets[t].copy()\n","  Y_test=Y_sets[t].copy()\n","  for i in range(times):\n","    if i==t :\n","      continue\n","    else:\n","      X_train=X_train.append(X_sets[i])\n","      Y_train=Y_train.append(Y_sets[i])\n","  tmp=Validation_MLE_poly(X_train, X_test, Y_train, Y_test, Max_poly_order)\n","  for i in range(Max_poly_order):\n","    count[i]+=tmp[i]\n","print(\"############################################\")\n","for i in range(Max_poly_order):\n","  print(\"average cross validation likelihood of M=\",i+1,\": \",count[i]/times)"],"metadata":{"id":"aEi-aPmm6ll7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d216e6d5-94cb-4625-b15f-0101bb3c44db","executionInfo":{"status":"ok","timestamp":1666008289542,"user_tz":-480,"elapsed":124133,"user":{"displayName":"蕭登方","userId":"04110943827274540537"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------time= 0 -------------------------\n","M= 1 avg log_likelihood [-1.07303505]\n","M= 2 avg log_likelihood [-1.05553961]\n","M= 3 avg log_likelihood [-5.20461925]\n","----------------time= 1 -------------------------\n","M= 1 avg log_likelihood [-0.87676809]\n","M= 2 avg log_likelihood [-0.90664619]\n","M= 3 avg log_likelihood [-0.93516671]\n","----------------time= 2 -------------------------\n","M= 1 avg log_likelihood [-1.02347907]\n","M= 2 avg log_likelihood [-1.16662856]\n","M= 3 avg log_likelihood [-1.00146888]\n","----------------time= 3 -------------------------\n","M= 1 avg log_likelihood [-0.98253852]\n","M= 2 avg log_likelihood [-1.00515208]\n","M= 3 avg log_likelihood [-1.15430724]\n","----------------time= 4 -------------------------\n","M= 1 avg log_likelihood [-1.15715304]\n","M= 2 avg log_likelihood [-1.22698465]\n","M= 3 avg log_likelihood [-1.31995545]\n","----------------time= 5 -------------------------\n","M= 1 avg log_likelihood [-0.95007759]\n","M= 2 avg log_likelihood [-0.97082579]\n","M= 3 avg log_likelihood [-0.90563096]\n","----------------time= 6 -------------------------\n","M= 1 avg log_likelihood [-1.00929481]\n","M= 2 avg log_likelihood [-0.92653587]\n","M= 3 avg log_likelihood [-1.02969768]\n","----------------time= 7 -------------------------\n","M= 1 avg log_likelihood [-0.86093816]\n","M= 2 avg log_likelihood [-0.78580016]\n","M= 3 avg log_likelihood [-1.23517368]\n","----------------time= 8 -------------------------\n","M= 1 avg log_likelihood [-1.11073442]\n","M= 2 avg log_likelihood [-1.13264739]\n","M= 3 avg log_likelihood [-1.25544504]\n","----------------time= 9 -------------------------\n","M= 1 avg log_likelihood [-1.00586958]\n","M= 2 avg log_likelihood [-1.13795849]\n","M= 3 avg log_likelihood [-1.68806333]\n","----------------time= 10 -------------------------\n","M= 1 avg log_likelihood [-1.0402195]\n","M= 2 avg log_likelihood [-1.20362339]\n","M= 3 avg log_likelihood [-4.69637346]\n","----------------time= 11 -------------------------\n","M= 1 avg log_likelihood [-0.86010494]\n","M= 2 avg log_likelihood [-0.8839524]\n","M= 3 avg log_likelihood [-0.93609008]\n","----------------time= 12 -------------------------\n","M= 1 avg log_likelihood [-1.11912483]\n","M= 2 avg log_likelihood [-1.06001226]\n","M= 3 avg log_likelihood [-1.11293299]\n","----------------time= 13 -------------------------\n","M= 1 avg log_likelihood [-0.83412731]\n","M= 2 avg log_likelihood [-0.81556335]\n","M= 3 avg log_likelihood [-0.88602054]\n","----------------time= 14 -------------------------\n","M= 1 avg log_likelihood [-1.23792336]\n","M= 2 avg log_likelihood [-1.40072089]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n"]},{"output_type":"stream","name":"stdout","text":["M= 3 avg log_likelihood [-inf]\n","----------------time= 15 -------------------------\n","M= 1 avg log_likelihood [-0.89451762]\n","M= 2 avg log_likelihood [-1.06810252]\n","M= 3 avg log_likelihood [-1.85180558]\n","############################################\n","average cross validation likelihood of M= 1 :  [-1.00224412]\n","average cross validation likelihood of M= 2 :  [-1.04666835]\n","average cross validation likelihood of M= 3 :  [-inf]\n"]}]},{"cell_type":"markdown","source":["After observing the result above\n","\n","I found that the over-fitting condition do occur for higher order polynomial regression like the \"-inf\" in time=14 M=3  \n","\n","So I conclude that M=1 order regression is the most suitable polynomial regression model for this problem"],"metadata":{"id":"1AHZCaDxljuV"}}]}